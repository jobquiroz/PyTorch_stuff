{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with PyTorch Step-by-Step: A Beginner's Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *\n",
    "\n",
    "# This is needed to render the plots in this chapter\n",
    "from plots.chapter5 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import Compose, Normalize\n",
    "\n",
    "from data_generation.image_classification import generate_dataset\n",
    "from helpers import index_splitter, make_balanced_sampler\n",
    "from stepbystep.v1 import StepByStep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutions\n",
    "\n",
    "A convolution is \"a mathematical operation on two functions (f and g) that produces a\n",
    "third function (f * g) expressing how the shape of one is modified by the other\"[87]. In\n",
    "image processing, a convolution matrix is also called a kernel or filter. Typical\n",
    "image processing operations, like blurring, sharpening, edge detection, and more are\n",
    "accomplished by performing a convolution between a kernel and an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter / Kernel\n",
    "\n",
    "Simply put, one defines a filter (or kernel, but we’re sticking with filter here), and\n",
    "applies this filter to an image (that is, convolving an image). Usually, the filters are\n",
    "small square matrices. The convolution itself is performed by applying the filter on\n",
    "the image repeatedly. Let’s try a concrete example to make it more clear.\n",
    "\n",
    "We’re using a single-channel image, and the most boring filter ever, the identity\n",
    "filter:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/conv1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the gray region on the top left corner of the image, which has the same size as\n",
    "the filter? That’s the region the filter is being applied to and it is called the\n",
    "receptive field, drawing an analogy to the way human vision works.\n",
    "\n",
    "Moreover, look at the shapes underneath the images: the shapes follow the NCHW\n",
    "shape convention used by PyTorch. There is one image, one channel, six by six\n",
    "pixels in size. There is one filter, one channel, three by three pixels in size.\n",
    "\n",
    "Finally, the asterisk is representing the convolution operation between the two.\n",
    "\n",
    "\n",
    "Let’s create Numpy arrays to follow the operations, after all, everything gets easier\n",
    "to understand in code, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 6, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single = np.array(\n",
    "    [[[[5, 0, 8, 7, 8, 1],\n",
    "       [1, 9, 5, 0, 7, 7],\n",
    "       [6, 0, 2, 4, 6, 6],\n",
    "       [9, 7, 6, 6, 8, 4],\n",
    "       [8, 3, 8, 5, 1, 3],\n",
    "       [7, 2, 7, 0, 1, 0]]]]\n",
    ")\n",
    "single.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 3, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identity = np.array(\n",
    "    [[[[0, 0, 0],\n",
    "       [0, 1, 0],\n",
    "       [0, 0, 0]]]]\n",
    ")\n",
    "identity.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolving\n",
    "\n",
    "> \"How does the filter modify the selected region/receptive field?\"\n",
    "\n",
    "It is actually quite simple: it performs an element-wise multiplication between the\n",
    "two, region and filter, and adds everything up. That’s it! Let’s check it out, zooming\n",
    "in on the selected region:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/conv2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In code, we have to slice the corresponding region (remember the NCHW shape, so we’re operating on the last two dimensions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region = single[:, :, 0:3, 0:3]\n",
    "filtered_region = region * identity\n",
    "total = filtered_region.sum()\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we’re done for the first region of the image!\n",
    "\n",
    "> \"Wait, there are nine pixel values coming in, but only ONE value\n",
    "coming out!\"\n",
    "\n",
    "Good point, you’re absolutely right! Doing a convolution produces an image with a\n",
    "reduced size. It is easy to see why, if we zoom out back to the full image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/conv3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the filter gets applied to the gray region, and we’re using an identity filter, it\n",
    "is fairly straightforward to see it is simply copying the value in the center of the\n",
    "region. The remaining values are simply multiplied by zero and do not make to the\n",
    "sum. But even if they did, it wouldn’t change the fact that the result of one\n",
    "operation is a single value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving Around"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we move the region one step to the right, that is, we change the receptive\n",
    "field, and apply the filter again:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/stride1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The size of the movement, in pixels, is called a stride. In our\n",
    "example, the stride is one.\n",
    "\n",
    "In code, it means we’re changing the slice of the input image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_region = single[:, :, 0:3, (0+1):(3+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the operation remains the same: first, an element-wise multiplication, and then\n",
    "adding up the elements of the resulting matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/conv5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_filtered_region = new_region * identity\n",
    "new_total = new_filtered_region.sum()\n",
    "new_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/conv6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can keep moving the gray region to the right until we can’t move it anymore:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/conv7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fourth step to the right will actually place the region partially outside the\n",
    "input image. That’s a big no-no!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_horizontal_region = single[:, :, 0:3, (0+4):(3+4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selected region does not match the shape of the filter anymore. So, if we try to\n",
    "perform the element-wise multiplication, it fails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1,1,3,2) (1,1,3,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_29884/2080732797.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlast_horizontal_region\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0midentity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1,1,3,2) (1,1,3,3) "
     ]
    }
   ],
   "source": [
    "last_horizontal_region * identity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we go back to the left side and move down one step. If we repeat the\n",
    "operation, covering all valid regions, we’ll end up with a resulting image that is\n",
    "smaller (on the right):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/conv8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It depends on the size of the filter.\n",
    "\n",
    "> The bigger the filter, the smaller the resulting image.\n",
    "\n",
    "Since applying a filter always produces a single value, the reduction is equal to the\n",
    "filter size minus one. If the input image has (hi, wi) shape (we’re disregarding the channel dimension for now), and the filter has (hf, wf) shape, the shape of the\n",
    "resulting image is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large\n",
    "(h_i, w_i) * (h_f, w_f) = (h_i - (h_f - 1), w_i - (w_f - 1))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume the filter is a square matrix of size f, we can simplify the expression\n",
    "above to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large\n",
    "(h_i, w_i) * f = (h_i - f + 1, w_i - f + 1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Makes sense, right? The filter has its dimensions reduced from (f, f) to (1, 1), so the\n",
    "operation reduces the original size by (f - 1).\n",
    "\n",
    "> \"But I’d like to keep the image size, is it possible?\"\n",
    "\n",
    "Sure it is! Padding comes to our rescue in this case. We’ll get to that in a couple of\n",
    "sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolving in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how a convolution works, let’s try it out using PyTorch. First, we\n",
    "need to convert our image and filter to tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.as_tensor(single).float()\n",
    "kernel_identity = torch.as_tensor(identity).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since kernel and filter are used interchangeably, especially when it comes to\n",
    "arguments of different methods, I am calling the variable kernel_identity, even\n",
    "though it is exactly the same identity filter we have used so far.\n",
    "\n",
    "\n",
    "Just like the activation functions we’ve seen in Chapter 4, convolutions also come\n",
    "in two flavors: **functional and module**. \n",
    "\n",
    "There is a fundamental difference between the two, though: the **functional** convolution takes the **kernel/filter as an argument** while **the module has weights** to represent the kernel/filter.\n",
    "\n",
    "Let’s use the functional convolution, `F.conv2d`, to apply the identity filter to our input image (notice we’re using `stride=1` since we moved the region around one\n",
    "pixel at a time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[9., 5., 0., 7.],\n",
       "          [0., 2., 4., 6.],\n",
       "          [7., 6., 6., 8.],\n",
       "          [3., 8., 5., 1.]]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolved = F.conv2d(image, kernel_identity, stride=1)\n",
    "convolved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we got the same result shown in the previous section. No surprises\n",
    "here.\n",
    "\n",
    "Now, let’s turn our attention to PyTorch’s convolution module, `nn.Conv2d`. It has\n",
    "many arguments, let’s focus on the first four of them:\n",
    "\n",
    "- `in_channels`: number of channels of the input image\n",
    "- `out_channels`: number of channels produced by the convolution\n",
    "- `kernel_size`: size of the (square) convolution filter/kernel\n",
    "- `stride`: the size of the movement of the selected region\n",
    "\n",
    "There is a couple of things to notice here. First, there is **no argument for the\n",
    "kernel/filter itself**, there is only a `kernel_size` argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The **actual filter**, that is, the **square matrix** used to perform\n",
    "element-wise multiplication is **learned** by the module.\n",
    "\n",
    "Second, it is possible to produce **multiple channels** as output. It simply means the\n",
    "module is going to **learn multiple filters**. Each filter is going to produce a different\n",
    "result, which is being called a channel here.\n",
    "\n",
    "So far, we’ve been using a single channel image as input, and applying one filter\n",
    "(size three by three) to it, moving one pixel at a time, resulting in one\n",
    "output/channel. Let’s do it in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-5.0736, -4.8809, -3.6007, -2.7029],\n",
       "          [-1.3333,  1.0643, -3.0053, -4.3564],\n",
       "          [-2.9651, -3.2726, -4.0595, -5.0429],\n",
       "          [-4.2851, -2.5307, -3.2834, -1.9497]]]],\n",
       "       grad_fn=<SlowConv2DBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1)\n",
    "\n",
    "conv(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are gibberish now (and yours are going to be different than mine)\n",
    "because the convolutional module randomly initializes the weights representing\n",
    "the kernel/filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> That’s the whole point of the convolutional module: it will learn\n",
    "the kernel/filter on its own.\n",
    "\n",
    "> In traditional computer vision, people would develop different\n",
    "filters for different purposes: blurring, sharpening, edge\n",
    "detection, and so on.\n",
    "\n",
    "> But, instead of being clever and trying to manually devise a filter\n",
    "that does the trick for a given problem, why not outsource the\n",
    "filter definition to the neural network as well? This way. the\n",
    "network will come up with filters that highlight features that are\n",
    "relevant to the task at hand.\n",
    "\n",
    "It’s no surprise that the resulting image shows a `grad_fn` attribute\n",
    "now: it will be used to compute gradients so the network can\n",
    "actually learn how to change the weights representing the filter.\n",
    "\n",
    "\n",
    "> \"Can we tell it to learn multiple filters at once?\"\n",
    "\n",
    "Sure we can, that’s the role of the `out_channels` argument. If we set it to two, it will\n",
    "generate two (randomly initialized) filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-0.0905,  0.1117,  0.2513],\n",
       "          [-0.2983, -0.2420, -0.2181],\n",
       "          [ 0.0585, -0.1858,  0.2686]]],\n",
       "\n",
       "\n",
       "        [[[-0.2958, -0.2141,  0.2932],\n",
       "          [ 0.2626, -0.3184,  0.0374],\n",
       "          [-0.2613, -0.2549,  0.3039]]]], requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_multiple = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=3, stride=1)\n",
    "conv_multiple.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See? There are **two** filters represented by **three-by-three** matrices of weights\n",
    "(your values are going to be different than mine).\n",
    "\n",
    "> Even if you have only one channel as input, you can have many\n",
    "channels as output.\n",
    "\n",
    "> **Spoiler alert**: the filters learned by the network are going to show\n",
    "edges, patterns, and even more complex shapes (sometimes\n",
    "resembling faces, for instance). We’ll get back to visualizing\n",
    "those filters later in this chapter.\n",
    "\n",
    "We can also **force** a convolutional module to use a particular filter by setting its\n",
    "weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    conv.weight[0] = kernel_identity\n",
    "    conv.bias[0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`weight[0]` and `bias[0]` are indexing the first (and only) output channel in this convolutional layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **IMPORTANT: setting the weights is a strictly no-gradient\n",
    "operation, so you should always use the no_grad context\n",
    "manager.**\n",
    "\n",
    "In the code snippet above, we are forcing the module to use the (boring) identity\n",
    "kernel we have used so far. As expected, if we convolve our input image we’ll get\n",
    "the familiar result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[9., 5., 0., 7.],\n",
       "          [0., 2., 4., 6.],\n",
       "          [7., 6., 6., 8.],\n",
       "          [3., 8., 5., 1.]]]], grad_fn=<SlowConv2DBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Setting the weights to get specific filters is at the heart of\n",
    "transfer learning. Someone else trained a model and that model\n",
    "learned lots of useful filters, so we don’t have to learn them\n",
    "again. We can set the corresponding weights and go from there.\n",
    "We’ll see this in practice in Chapter 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Striding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we’ve been moving the region of interest one pixel at a time: a stride of one.\n",
    "Let’s try a **stride of two** for a change and see what happens to the resulting image. I\n",
    "am not reproducing the first step here because it is always the same: the gray\n",
    "region centered at the number nine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/strider2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **second step**, depicted above, shows the **gray region** moved two pixels to the\n",
    "right: that’s a stride of two.\n",
    "\n",
    "Moreover, notice that, if we take another step of two pixels, the gray region would be\n",
    "placed partially outside the underlying image. It was and still is a big no-no, so\n",
    "there are **only two valid operations** while moving horizontally. The same will\n",
    "eventually happen when we move vertically. The first stride of two pixels down is\n",
    "fine, but the second will be, once again, a failed operation.\n",
    "\n",
    "The resulting image, after the only four valid operations, looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/strider3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The identity kernel may be boring, but it is definitely **useful** to highlight the inner\n",
    "workings of the convolutions. It is crystal clear in the figure above where the pixel\n",
    "values in the resulting image come from.\n",
    "\n",
    "Also, notice that using a **bigger stride** made the shape of the resulting image even\n",
    "smaller.\n",
    "\n",
    "> The **bigger the stride**, the **smaller the resulting image**.\n",
    "\n",
    "Once again, it makes sense: if we are **skipping pixels** in the input image, there are\n",
    "fewer regions of interest to apply the filter to. We can extend our previous formula\n",
    "to include the stride size (s):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large\n",
    "(h_i, w_i) * f = \\left(\\frac{h_i - f + 1}{s}, \\frac{w_i - f + 1}{s}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we’ve seen before, the stride is only an argument of the convolution, so let’s use\n",
    "PyTorch’s functional convolution to double-check the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[9., 0.],\n",
       "          [7., 6.]]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolved_stride2 = F.conv2d(image, kernel_identity, stride=2)\n",
    "convolved_stride2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, the operations we performed have been shrinking the images. What about\n",
    "restoring them to their original glory, I mean, size?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding means stuffing. We need to stuff the original image so it can sustain the\n",
    "\"attack\" on its size.\n",
    "\n",
    "> \"How do I stuff an image?\"\n",
    "\n",
    "Glad you asked! We may simply **add zeros around it**. An image is worth a thousand words in this case:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/padding1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what I mean? By adding columns and rows of zeros around it, **we expand the\n",
    "input image** such that the gray region starts centered in the actual top left corner\n",
    "of the input image. This simple trick can be used to **preserve the original size** of the\n",
    "image.\n",
    "\n",
    "In code, as usual, PyTorch gives us two options: functional (`F.pad`) and module\n",
    "(`nn.ConstantPad2d`). Let’s start with the module version this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 5., 0., 8., 7., 8., 1., 0.],\n",
       "          [0., 1., 9., 5., 0., 7., 7., 0.],\n",
       "          [0., 6., 0., 2., 4., 6., 6., 0.],\n",
       "          [0., 9., 7., 6., 6., 8., 4., 0.],\n",
       "          [0., 8., 3., 8., 5., 1., 3., 0.],\n",
       "          [0., 7., 2., 7., 0., 1., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constant_padder = nn.ConstantPad2d(padding=1, value=0)\n",
    "constant_padder(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two arguments: `padding`, for the number of columns and rows to be\n",
    "stuffed in the image; and value, for the value that is filling these new columns and\n",
    "rows. One can also do **asymmetric padding**, by specifying a tuple in the padding\n",
    "argument representing (left, right, top, bottom). So, if we were to stuff our\n",
    "image on left and right sides only, the argument would go like this: (1, 1, 0, 0).\n",
    "\n",
    "We can achieve the same result using the functional padding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 5., 0., 8., 7., 8., 1., 0.],\n",
       "          [0., 1., 9., 5., 0., 7., 7., 0.],\n",
       "          [0., 6., 0., 2., 4., 6., 6., 0.],\n",
       "          [0., 9., 7., 6., 6., 8., 4., 0.],\n",
       "          [0., 8., 3., 8., 5., 1., 3., 0.],\n",
       "          [0., 7., 2., 7., 0., 1., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = F.pad(image, pad=(1, 1, 1, 1), mode='constant', value=0)\n",
    "\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the functional version, one must specify the **padding as a tuple**. The value\n",
    "argument is straightforward, and there is yet another argument: `mode`, which was\n",
    "set to **constant** to match the module version above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In PyTorch’s documentation there is a **note** warning about\n",
    "possible reproducibility issues while using padding:\n",
    "\n",
    "> *\"When using the CUDA backend, this operation may induce\n",
    "nondeterministic behaviour in its backward pass that is not easily\n",
    "switched off. Please see the notes on Reproducibility for background.\"*\n",
    "It strikes me a bit odd that such a straightforward operation, of all\n",
    "things, would jeopardize reproducibility. Go figure!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"What are the other available modes?\"\n",
    "\n",
    "There are three other modes: replicate, reflect, and circular. Let’s take a look at\n",
    "them, starting with the visualization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/paddings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the replication padding, the padded pixels will have the same value as the\n",
    "closest real pixel. The padded corners will have the same value as the real corners.\n",
    "The other columns (left and right) and rows (top and bottom) will replicate the\n",
    "corresponding values of the original image. The values used in the replication are in\n",
    "a darker shade of orange.\n",
    "\n",
    "In PyTorch, one can use the functional form `F.pad` with `mode=\"replicate\"`, or use\n",
    "the module version `nn.ReplicationPad2d`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[5., 5., 0., 8., 7., 8., 1., 1.],\n",
       "          [5., 5., 0., 8., 7., 8., 1., 1.],\n",
       "          [1., 1., 9., 5., 0., 7., 7., 7.],\n",
       "          [6., 6., 0., 2., 4., 6., 6., 6.],\n",
       "          [9., 9., 7., 6., 6., 8., 4., 4.],\n",
       "          [8., 8., 3., 8., 5., 1., 3., 3.],\n",
       "          [7., 7., 2., 7., 0., 1., 0., 0.],\n",
       "          [7., 7., 2., 7., 0., 1., 0., 0.]]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replication_padder = nn.ReplicationPad2d(padding=1)\n",
    "replication_padder(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the **reflection** padding, it gets a bit trickier. It is like **the outer columns and rows\n",
    "are used as axes for the reflection**. \n",
    "\n",
    "So, the left padded column (forget about the corners for now) will reflect the second column (since the first column is the axis of reflection). The same reasoning goes for the right padded column. Similarly, the top\n",
    "padded row will reflect the second row (since the first row is the axis of reflection),\n",
    "and the same reasoning goes for the bottom padded row. The values used in the\n",
    "reflection are in a darker shade of orange. The corners will have the same values as\n",
    "the intersection of the reflected rows and columns of the original image.\n",
    "\n",
    "Hopefully, the image can convey the idea better than my words.\n",
    "\n",
    "In PyTorch, you can use the functional form F.pad with mode=\"reflect\", or use the\n",
    "module version nn.ReflectionPad2d:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[9., 1., 9., 5., 0., 7., 7., 7.],\n",
       "          [0., 5., 0., 8., 7., 8., 1., 8.],\n",
       "          [9., 1., 9., 5., 0., 7., 7., 7.],\n",
       "          [0., 6., 0., 2., 4., 6., 6., 6.],\n",
       "          [7., 9., 7., 6., 6., 8., 4., 8.],\n",
       "          [3., 8., 3., 8., 5., 1., 3., 1.],\n",
       "          [2., 7., 2., 7., 0., 1., 0., 1.],\n",
       "          [3., 8., 3., 8., 5., 1., 3., 1.]]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reflection_padder = nn.ReflectionPad2d(padding=1)\n",
    "reflection_padder(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the circular padding, the left-most (right-most) column gets copied as the right\n",
    "(left) padded column (forget about the corners for now too). Similarly, the topmost\n",
    "(bottom-most) row gets copied as the bottom (top) padded row. The corners\n",
    "will receive the values of the diametrically opposed corner: the top-left padded\n",
    "pixel receives the value of the bottom-right corner of the original image. Once\n",
    "again, the values used in the padding are in a darker shade of orange.\n",
    "\n",
    "In PyTorch, you must use the functional form F.pad with mode=\"circular\" since\n",
    "there is no module version of the circular padding (at time of writing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 7., 2., 7., 0., 1., 0., 7.],\n",
       "          [1., 5., 0., 8., 7., 8., 1., 5.],\n",
       "          [7., 1., 9., 5., 0., 7., 7., 1.],\n",
       "          [6., 6., 0., 2., 4., 6., 6., 6.],\n",
       "          [4., 9., 7., 6., 6., 8., 4., 9.],\n",
       "          [3., 8., 3., 8., 5., 1., 3., 8.],\n",
       "          [0., 7., 2., 7., 0., 1., 0., 7.],\n",
       "          [1., 5., 0., 8., 7., 8., 1., 5.]]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.pad(image, pad=(1, 1, 1, 1), mode='circular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By means of **padding** an image, it is possible to **get resulting images** with the same\n",
    "shape as input images, or even bigger, should you choose to stuff more and more\n",
    "rows and columns to the input image. Assuming we’re doing **symmetrical padding\n",
    "of size p**, the resulting shape is given by the formula below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large\n",
    "(h_i, w_i) * f = \\left(\\frac{(h_i + 2p) - f + 1}{s}, \\frac{(w_i + 2p) - f + 1}{s}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re basically extending the original dimensions by 2p pixels each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A REAL Filter\n",
    "\n",
    "Enough with the identity filter! Let’s try an **edge detector**[88] filter from traditional\n",
    "computer vision for a change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge = np.array(\n",
    "    [[[[0, 1, 0],\n",
    "       [1, -4, 1],\n",
    "       [0, 1, 0]]]]\n",
    ")\n",
    "kernel_edge = torch.as_tensor(edge).float()\n",
    "kernel_edge.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let’s apply it to a different region of our (padded) input image as well:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/padding2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, filters, other than the identity one, will not simply copy the value at\n",
    "the center. The element-wise multiplication finally means something:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/padding3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded = F.pad(image, (1, 1, 1, 1), mode='constant', value=0)\n",
    "conv_padded = F.conv2d(padded, kernel_edge, stride=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooling\n",
    "\n",
    "Now we’re back in the business of **shrinking images**. Pooling is different than the\n",
    "former operations: it splits the image into tiny chunks, performs an operation on\n",
    "each chunk (that yields a single value), and puts the chunks together as the\n",
    "resulting image. Again, an image is worth a thousand words:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/pooling1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the image above, we’re performing a **max-pooling** with a kernel size of two.\n",
    "\n",
    "Even though these are not quite the same filters as the ones we’ve already seen, it\n",
    "is still called kernel.\n",
    "\n",
    "> In this example, the stride is assumed to be the same size as the\n",
    "kernel.\n",
    "\n",
    "Our input image is split into nine chunks, and we perform a simple max operation\n",
    "(hence, max-pooling) on each chunk (really, it is just taking the biggest value in each\n",
    "chunk). Then these values are put together, in order, to produce a smaller resulting\n",
    "image.\n",
    "\n",
    "> The bigger the pooling kernel, the smaller the resulting image.\n",
    "\n",
    "A pooling kernel of two-by-two results in an image whose dimensions are half of\n",
    "the original. A pooling kernel of three-by-three makes the resulting image one third the size of the original, and so on. Moreover, **only full chunks count**: if we try a\n",
    "kernel of four-by-four in our six-by-six image, only one chunk fits, and the resulting\n",
    "image would have a single pixel.\n",
    "\n",
    "In PyTorch, as usual, we have both forms: `F.max_pool2d` and `nn.MaxPool2d`. Let’s\n",
    "use functional form to replicate the max-pooling in the figure above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[22., 23., 11.],\n",
       "          [24.,  7.,  1.],\n",
       "          [13., 13., 13.]]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled = F.max_pool2d(conv_padded, kernel_size=2)\n",
    "pooled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then let’s use the module version to illustrate the big four-by-four pooling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[24.]]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxpool4 = nn.MaxPool2d(kernel_size=4)\n",
    "pooled4 = maxpool4(conv_padded)\n",
    "pooled4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single pixel as promised!\n",
    "\n",
    "> \"Can I perform some other operation?\"\n",
    "\n",
    "Sure, besides **max-pooling**, **average pooling** is also fairly common. As the name\n",
    "suggests, it will output the average pixel value for each chunk. In PyTorch, we have\n",
    "`F.avg_pool2d` and `nn.AvgPool2d`.\n",
    "\n",
    "> \"Can I use a stride of a different size?\"\n",
    "\n",
    "Of course, you can! In this case, there will be an **overlap** between regions instead of\n",
    "a clean split into chunks. So, it looks like a regular kernel of a convolution, but the\n",
    "operation is already defined (max or average, for instance). Let’s go through a\n",
    "quick example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[24., 24., 23., 23.],\n",
       "          [24., 24., 23., 23.],\n",
       "          [24., 24., 13., 13.],\n",
       "          [13., 13., 13., 13.]]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.max_pool2d(conv_padded, kernel_size=3, stride=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The max-pooling kernel, sized three-by-three, will move over the image (just like\n",
    "the convolutional kernel) and compute the maximum value of each region it goes\n",
    "over. The resulting shape follows the formula on Equation 5.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flattening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve already seen this one! It simply flattens a tensor, preserving the first\n",
    "dimension such that we keep the number of data points while collapsing all other\n",
    "dimensions. It has a module version nn.Flatten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[22., 23., 11., 24.,  7.,  1., 13., 13., 13.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened = nn.Flatten()(pooled)\n",
    "flattened"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has no functional version, but there is no need for one since we can accomplish\n",
    "the same thing using view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled.view(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensions\n",
    "\n",
    "We’ve performed convolutions, padding, and pooling in **two dimensions** because\n",
    "we’re handling images. But there are one and three-dimensional versions of some\n",
    "of them as well:\n",
    "\n",
    "- `nn.Conv1d` and `F.conv1d`; `nn.Conv3d` and `F.conv3d`\n",
    "- `nn.ConstandPad1d` and `nn.ConstandPad3d`\n",
    "- `nn.ReplicationPad1d` and `nn.ReplicationPad3d`\n",
    "- `nn.ReflectionPad1d`\n",
    "- `nn.MaxPool1d` and `F.max_pool1d`; `nn.MaxPool3d` and `F.max_pool3d`\n",
    "- `nn.AvgPool1d` and `F.avg_pool1d`; `nn.AvgPool3d` and `F.avg_pool3d`\n",
    "\n",
    "We will not venture into the third dimension in this book, but we’ll get back to one-dimension\n",
    "operations later.\n",
    "\n",
    "- \"Aren’t color images three-dimensional since they have three channels?\"\n",
    "\n",
    "Well, yes, but we will still be applying two-dimensional convolutions to them. We’ll\n",
    "go through a detailed example using a three-channel image in the next chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Typical Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typical architecture uses a sequence of one or more typical convolutional\n",
    "blocks, each block consisting of three operations:\n",
    "\n",
    "1. Convolution\n",
    "2. Activation function\n",
    "3. Pooling\n",
    "\n",
    "As images go through these operations, they will shrink in size. After three of these\n",
    "blocks (assuming kernel size of two for pooling), for instance, an image will be\n",
    "reduced to 1/8 or less of its original dimensions (and thus 1/64 of its total number\n",
    "of pixels). The number of channels/filters produced by each block, though, is\n",
    "usually increased as more blocks are added.\n",
    "\n",
    "After the sequence of blocks, the image gets flattened: hopefully, at this stage,\n",
    "there is no loss of information by considering each value in the flattened tensor a\n",
    "feature on its own.\n",
    "\n",
    "Once the features are dissociated from pixels, it becomes a fairly standard\n",
    "problem, like the ones we’ve been handling in this book: the features feed one or\n",
    "more hidden layers, and an output layer produces the logits for classification.\n",
    "\n",
    "\n",
    "> If you think of it, what those typical convolutional blocks do is\n",
    "akin to pre-processing images and converting them into\n",
    "features. Let’s call this part of the network **a featurizer** (the one\n",
    "that generates features).\n",
    "\n",
    "> The classification itself is handled by the familiar and well-known\n",
    "hidden and output layers.\n",
    "\n",
    "> In transfer learning, which we’ll see in Chapter 7, this will\n",
    "become even more clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet-5 is a 7-level convolutional neural network developed by Yann LeCun in\n",
    "1998 to recognize hand-written digits in 28x28 pixel images - the famous **MNIST**\n",
    "dataset! That’s when it all started (kinda). \n",
    "\n",
    "In 1989, LeCun himself used backpropagation (chained gradient descent, remember?) to learn the convolution\n",
    "filters, as we discussed above, instead of painstakingly developing them manually.\n",
    "\n",
    "His network had this architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/architecture_lenet.png)\n",
    "\n",
    "*Source: Generated using Alexander Lenail's [NN-SVG](http://alexlenail.me/NN-SVG/) and adapted by the author. For more details, see LeCun, Y., et al (1998).  [Gradient-based learning applied to document recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf). Proceedings of the IEEE,86(11), 2278–2324*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see anything familiar? The typical convolutional blocks are already there\n",
    "(to some extent): convolutions (C layers), activation functions (not shown), and\n",
    "subsampling (S layers). There are some differences, though:\n",
    "\n",
    "- back then, the subsampling was more complex than today’s max-pooling, but the general idea still holds\n",
    "\n",
    "- the activation function, a sigmoid at the time, was applied after the subsampling instead of before, as it is typical today\n",
    "\n",
    "- the F6 and OUTPUT layers were connected by something called \"Gaussian connections\", which is also more complex than the typical activation function one would use today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapting LeNet-5 to today’s standards, it could be implemented like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet = nn.Sequential()\n",
    "\n",
    "# Featurizer\n",
    "# Block 1: 1@28x28 -> 6@28x28 -> 6@14x14\n",
    "lenet.add_module('C1', nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2))\n",
    "lenet.add_module('func1', nn.ReLU())\n",
    "lenet.add_module('S2', nn.MaxPool2d(kernel_size=2))\n",
    "\n",
    "# Block 2: 6@14x14 -> 16@10x10 -> 16@5x5\n",
    "lenet.add_module('C3', nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5))\n",
    "lenet.add_module('func2', nn.ReLU())\n",
    "lenet.add_module('S4', nn.MaxPool2d(kernel_size=2))\n",
    "\n",
    "# Block 3: 16@5x5 -> 120@1x1\n",
    "lenet.add_module('C5', nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5))\n",
    "lenet.add_module('func2', nn.ReLU())\n",
    "\n",
    "# Flattening\n",
    "lenet.add_module('flatten', nn.Flatten())\n",
    "\n",
    "# Classification\n",
    "# Hidden Layer\n",
    "lenet.add_module('F6', nn.Linear(in_features=120, out_features=84))\n",
    "lenet.add_module('func3', nn.ReLU())\n",
    "# Output Layer\n",
    "lenet.add_module('OUTPUT', nn.Linear(in_features=84, out_features=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet-5 used three convolutional blocks, although the last one does not have a max-pooling because the convolution already produces a single pixel. Regarding the number of channels, they increase as the image size decreases:\n",
    "\n",
    "- input image: single-channel 28x28 pixels\n",
    "- first block: produces 6-channel 14x14 pixels\n",
    "- second block: produces 16-channel 5x5 pixels\n",
    "- third block: produces 120-channel single-pixel (1x1)\n",
    "\n",
    "Then, these 120 values (or features) are flattened and fed to a typical hidden layer\n",
    "with 84 units. The last step is, obviously, the output layer, which produces 10 logits\n",
    "to be used for digit classification (from 0 to 9, there are 10 classes).\n",
    "\n",
    "> \"Wait, we haven’t seen any of those multiclass classification problems yet!\"\n",
    "\n",
    "You’re right, it is about time. But we’re still not using MNIST for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Multiclass Classification Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A problem is considered a multiclass classification problem if there are more than\n",
    "two classes. So, let’s keep it as simple as possible and build a model to classify\n",
    "images into three classes."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "Our images are going to have either a diagonal or a parallel line, BUT this time we\n",
    "will make a distinction between a diagonal line tilted to the right, a diagonal line\n",
    "tilted to the left, and a parallel line (it doesn’t matter if it is horizontal or vertical).\n",
    "We can summarize the labels (y) like this:\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let’s generate more and bigger images: one thousand images, each one ten-by-\n",
    "ten pixels in size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = generate_dataset(img_size=10, n_images=1000, binary=False, seed=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x324 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plot_images(images, labels, n_plot=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data preparation step would be identical to the one we used in Chapter 4 if it\n",
    "weren’t for one change: we will not perform data augmentation this time.\n",
    "\n",
    "> \"Why not?\"\n",
    "\n",
    "In our particular problem, flipping an image is potentially ruining the label. If we\n",
    "have an image containing a diagonal line tilted to the right (thus labeled as class\n",
    "index #1), and we flip it, the diagonal line would end up tilted to the left. But data\n",
    "augmentation does not change the labels, so the result is an image with a wrong\n",
    "label (class index #1, even though it would contain a left-tilted diagonal line).\n",
    "\n",
    "> Data augmentation may be useful, but it should not produce\n",
    "images that are inconsistent with their labels.\n",
    "\n",
    "That being said, we’re only keeping the min-max scaling using the Normalize\n",
    "transform. All the rest remains the same: splitting, datasets, sampler, and data\n",
    "loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformedTensorDataset(Dataset):\n",
    "    def __init__(self, x, y, transform=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.x[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        \n",
    "        return x, self.y[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds tensors from numpy arrays BEFORE split\n",
    "# Modifies the scale of pixel values from [0, 255] to [0, 1]\n",
    "x_tensor = torch.as_tensor(images / 255).float()\n",
    "y_tensor = torch.as_tensor(labels).long()\n",
    "\n",
    "# Uses index_splitter to generate indices for training and\n",
    "# validation sets\n",
    "train_idx, val_idx = index_splitter(len(x_tensor), [80, 20])\n",
    "# Uses indices to perform the split\n",
    "x_train_tensor = x_tensor[train_idx]\n",
    "y_train_tensor = y_tensor[train_idx]\n",
    "x_val_tensor = x_tensor[val_idx]\n",
    "y_val_tensor = y_tensor[val_idx]\n",
    "\n",
    "# We're not doing any data augmentation now\n",
    "train_composer = Compose([Normalize(mean=(.5,), std=(.5,))])\n",
    "val_composer = Compose([Normalize(mean=(.5,), std=(.5,))])\n",
    "\n",
    "# Uses custom dataset to apply composed transforms to each set\n",
    "train_dataset = TransformedTensorDataset(x_train_tensor, y_train_tensor, transform=train_composer)\n",
    "val_dataset = TransformedTensorDataset(x_val_tensor, y_val_tensor, transform=val_composer)\n",
    "\n",
    "# Builds a weighted random sampler to handle imbalanced classes\n",
    "sampler = make_balanced_sampler(y_train_tensor)\n",
    "\n",
    "# Uses sampler in the training set to get a balanced data loader\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16, sampler=sampler)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "New problem, new loss. Since we’re embracing multiclass classification now, we\n",
    "need to use a different loss. And, once again, it all starts with our \"favorite\" subject:\n",
    "logits.\n",
    "\n",
    "#### Logits\n",
    "\n",
    "In binary classification problems, the model would produce one logit, and one logit\n",
    "only, for each data point. It makes sense, binary classification is about answering a\n",
    "simple question: \"does a given data point belong to the positive class?\".\n",
    "\n",
    "The **logit** output represented the log odds ratio (remember that?) of answering\n",
    "\"yes\" to the question above. The log odds ratio of a \"no\" answer was simply the\n",
    "inverse. There was no need to pose any other question to make a decision. And we\n",
    "used a sigmoid function to map logits to probabilities. It was a simple world :-)\n",
    "But a multiclass classification is more complex: we need to ask more questions,\n",
    "that is, **we need to get log odds ratios for every possible class. In other words, we\n",
    "need as many logits as there are classes.**\n",
    "\n",
    "> \"But a sigmoid takes only one logit. I guess we need something else to get probabilities, right?\"\n",
    "\n",
    "Absolutely correct! The function we’re looking for here is called **softmax**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax function returns, for each class, the contribution that a given class\n",
    "had to the **sum of odds ratios**. The class with a higher odds ratio will have the\n",
    "biggest contribution and thus the **highest probability**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Since the **softmax** is computed using odds ratios instead of log\n",
    "odds ratios (logits), we need to exponentiate the logits!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large\n",
    "\\begin{array}\n",
    "& z & = \\text{logit}(p) & = \\text{log odds ratio }(p) & = \\text{log}\\left(\\frac{p}{1-p}\\right)\n",
    "\\\\\n",
    "e^z & = e^{\\text{logit}(p)} & = \\text{odds ratio }(p) & = \\left(\\frac{p}{1-p}\\right)\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax formula itself is quite simple:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large\n",
    "\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{c=0}^{N_c-1}{e^{z_c}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the equation above, C stands for the number of classes and i corresponds to the\n",
    "index of a particular class. In our example, we have three classes, so our model\n",
    "needs to output three logits (z0, z1, z2). Applying softmax to these logits, we would\n",
    "get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large\n",
    "\\text{softmax}(z) = \\left[\\frac{e^{z_0}}{e^{z_0}+e^{z_1}+e^{z_2}},\\frac{e^{z_1}}{e^{z_0}+e^{z_1}+e^{z_2}},\\frac{e^{z_2}}{e^{z_0}+e^{z_1}+e^{z_2}}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple, right? Let’s see it in code now. Assuming our model produces this tensor\n",
    "containing three logits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.tensor([ 1.3863,  0.0000, -0.6931])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We exponentiate the logits to get the corresponding odds ratios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.0000, 1.0000, 0.5000])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odds_ratios = torch.exp(logits)\n",
    "odds_ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting tensor is telling us that the first class **has much better odds** than the other two, and the second one has better odds than the third. So we take these\n",
    "odds and add them together, and then compute the each class' contribution to the\n",
    "sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7273, 0.1818, 0.0909])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmaxed = odds_ratios / odds_ratios.sum()\n",
    "softmaxed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voilà! Our logits were softmaxed: the probabilities are proportional to the odds\n",
    "ratios. This data point most likely belongs to the first class since it has a probability\n",
    "of 72.73%.\n",
    "But there is absolutely no need to compute it manually, of course. PyTorch\n",
    "provides the typical implementations: functional (F.softmax) and module\n",
    "(nn.Softmax):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Softmax(dim=-1)(logits), F.softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases, it asks you to **provide which dimension** the softmax function should\n",
    "be applied to. In general, our models will produce logits with the shape (number of\n",
    "data points, number of classes), so **the right dimension to apply softmax to is the\n",
    "last one (dim=-1).**\n",
    "\n",
    "#### LogSoftmax.\n",
    "\n",
    "The logsoftmax function returns, well, the logarithm of the softmax function\n",
    "above. But, instead of manually taking the logarithm, PyTorch provides\n",
    "`F.log_softmax` and `nn.LogSoftmax` out of the box.\n",
    "\n",
    "These functions are **faster** and also have **better numerical properties**. But, I guess your main question at this point is:\n",
    "\n",
    "> \"Why do I need to take the log of the softmax?\"\n",
    "\n",
    "The simple and straightforward reason is that **the loss function expects log-probabilities as input.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Log Likelihood Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the softmax returns **probabilities**, the logsoftmax returns **log-probabilities**. And that’s the input for computing the **negative log-likelihood loss**, or `NLLLoss` for short. \n",
    "\n",
    "This loss is simply an extension of the binary cross-entropy loss to handle multiple classes.\n",
    "\n",
    "This was the formula for computing binary cross-entropy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large\n",
    "\\texttt{BCE}(y)={-\\frac{1}{(N_{\\text{pos}}+N_{\\text{neg}})}\\Bigg[{\\sum_{i=1}^{N_{\\text{pos}}}{\\text{log}(\\text{P}(y_i=1))} + \\sum_{i=1}^{N_{\\text{neg}}}{\\text{log}(1 - \\text{P}(y_i=1))}}\\Bigg]}\n",
    "\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the log-probabilities in the summation terms? In our example, there are three\n",
    "classes, that is, our labels (y) could be either zero, one, or two. So, the loss function\n",
    "will look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large\n",
    "\\texttt{NLLLoss}(y)={-\\frac{1}{(N_0+N_1+N_2)}\\Bigg[{\\sum_{i=1}^{N_0}{\\text{log}(\\text{P}(y_i=0))} + \\sum_{i=1}^{N_1}{\\text{log}(\\text{P}(y_i=1))} + \\sum_{i=1}^{N_2}{\\text{log}(\\text{P}(y_i=2))}}\\Bigg]}\n",
    "\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take, for instance, the first class (**y=0**). For every data point belonging to this class\n",
    "(there are **N0** of them), we take the logarithm of the predicted probability for that\n",
    "point and class *(log(P(yi=0)))* and add them all up. Next, we repeat the process for\n",
    "the other two classes, add all three results up, and divide by the total number of\n",
    "data points.\n",
    "\n",
    "> The loss only considers the predicted probability for the true\n",
    "class.\n",
    "\n",
    "If a data point is labeled as belonging to class index two, the loss will consider the\n",
    "probability assigned to class index two only. The other probabilities will be\n",
    "completely ignored.\n",
    "\n",
    "For a total of C classes, the formula can be written like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\\\\n",
    "\\Large \\texttt{NLLLoss}(y)={-\\frac{1}{(N_0+\\cdots+N_{C-1})}\\sum_{c=0}^{C-1}{\\sum_{i=1}^{N_c}{\\text{log}(\\text{P}(y_i=c))} }}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the **log-probabilities** are obtained by applying **logsoftmax**, this loss isn’t\n",
    "doing much more than looking up the inputs corresponding to the true class and\n",
    "adding them up. Let’s see this in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3185, -1.7048, -2.3979])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probs = F.log_softmax(logits, dim=-1)\n",
    "log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the log-probabilities for each class we computed using logsoftmax for\n",
    "our single data point. Now, let’s assume its label is two: what is the corresponding\n",
    "loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3979)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = torch.tensor([2])\n",
    "F.nll_loss(log_probs.view(-1, 3), label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is the **negative** of the **log-probability** corresponding to the class index (two) of\n",
    "the true label.\n",
    "\n",
    "As you’ve probably noticed, I used the **functional** version of the loss in the snippet of\n",
    "code above: F.nll_loss. But, as we’ve done with the binary cross-entropy loss in\n",
    "Chapter 3, we’re likely using the module version: nn.NLLLoss.\n",
    "\n",
    "Just like before, this loss function is a higher-order function, and this one takes\n",
    "three optional arguments (the others are deprecated and you can safely ignore\n",
    "them):\n",
    "\n",
    "- `reduction`: it takes either mean, sum, or none. The default, mean, corresponds to our Equation 5.10 above. As expected, sum will return the sum of the errors, instead of the average. The last option, none, corresponds to the unreduced form, that is, it returns the full array of errors.\n",
    "\n",
    "- `weight`: it takes a tensor of length C, that is, containing as many weights as there are classes.\n",
    "\n",
    "> IMPORTANT: this argument can be used to **handle imbalanced\n",
    "datasets**, unlike the weight argument in the binary cross-entropy\n",
    "losses we’ve seen in Chapter 3.\n",
    "Also, unlike the `pos_weight` argument of `BCEWithLogitsLoss`, the\n",
    "NLLLoss computes a true weighted average when this argument\n",
    "is used.\n",
    "\n",
    "- ignore_index: it takes one integer, corresponding to the one (and only one) class index that should be ignored when computing the loss. It can be used to mask a particular label that is not relevant to the classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s go through some quick examples using the arguments above. First, we need to\n",
    "generate some dummy logits (we’ll keep using three classes, though), and the\n",
    "corresponding log-probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5229, -0.3146, -2.9600],\n",
       "        [-1.7934, -1.0044, -0.7607],\n",
       "        [-1.2513, -1.0136, -1.0471],\n",
       "        [-2.6799, -0.2219, -2.0367],\n",
       "        [-1.0728, -1.9098, -0.6737]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(11)\n",
    "dummy_logits = torch.randn((5, 3))\n",
    "#print(dummy_logits)\n",
    "dummy_labels = torch.tensor([0, 0, 1, 2, 1])\n",
    "dummy_log_probs = F.log_softmax(dummy_logits, dim=-1)\n",
    "dummy_log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Can you hand-pick the log-probabilities that are going to be\n",
    "actually used in the loss computation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6553)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_log_probs = torch.tensor([-1.5229, -1.7934, -1.0136, -2.0367, -1.9098])\n",
    "-relevant_log_probs.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s use `nn.NLLLoss` to create the actual loss function, and then use\n",
    "predictions and labels to check if we got the relevant log-probabilities right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6553)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = nn.NLLLoss()\n",
    "loss_fn(dummy_log_probs, dummy_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right indeed! What if we want to balance our dataset, giving data points with label\n",
    "(y=2) double the weight of the other classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7188)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = nn.NLLLoss(weight=torch.tensor([1., 1., 2.]))\n",
    "loss_fn(dummy_log_probs, dummy_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what if we want to simply ignore data points with label (y=2)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5599)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = nn.NLLLoss(ignore_index=2)\n",
    "loss_fn(dummy_log_probs, dummy_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, once again, there is yet another loss function available for multiclass\n",
    "classification. And, once again, it is very important to know when to use one or the\n",
    "other, so you don’t end up with an inconsistent combination of model and loss\n",
    "function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAGINA 433\n",
    "\n",
    "### Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(11)\n",
    "dummy_logits = torch.randn((5, 3))\n",
    "dummy_labels = torch.tensor([0, 0, 1, 2, 1])\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn(dummy_logits, dummy_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "model_cnn1 = nn.Sequential()\n",
    "\n",
    "# Featurizer\n",
    "# Block 1: 1@10x10 -> n_channels@8x8 -> n_channels@4x4\n",
    "n_channels = 1\n",
    "model_cnn1.add_module('conv1', nn.Conv2d(in_channels=1, out_channels=n_channels, kernel_size=3))\n",
    "model_cnn1.add_module('relu1', nn.ReLU())\n",
    "model_cnn1.add_module('maxp1', nn.MaxPool2d(kernel_size=2))\n",
    "# Flattening: n_channels * 4 * 4\n",
    "model_cnn1.add_module('flatten', nn.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification\n",
    "# Hidden Layer\n",
    "model_cnn1.add_module('fc1', nn.Linear(in_features=n_channels*4*4, out_features=10))\n",
    "model_cnn1.add_module('relu2', nn.ReLU())\n",
    "# Output Layer\n",
    "model_cnn1.add_module('fc2', nn.Linear(in_features=10, out_features=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/classification_softmax.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "multi_loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer_cnn1 = optim.SGD(model_cnn1.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbs_cnn1 = StepByStep(model_cnn1, multi_loss_fn, optimizer_cnn1)\n",
    "sbs_cnn1.set_loaders(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbs_cnn1.train(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sbs_cnn1.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Filters and More!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def _visualize_tensors(axs, x, y=None, yhat=None, \n",
    "                       layer_name='', title=None):\n",
    "    # The number of images is the number of subplots in a row\n",
    "    n_images = len(axs)\n",
    "    # Gets max and min values for scaling the grayscale\n",
    "    minv, maxv = np.min(x[:n_images]), np.max(x[:n_images])\n",
    "    # For each image\n",
    "    for j, image in enumerate(x[:n_images]):\n",
    "        ax = axs[j]\n",
    "        # Sets title, labels, and removes ticks\n",
    "        if title is not None:\n",
    "            ax.set_title(f'{title} #{j}', fontsize=12)\n",
    "        shp = np.atleast_2d(image).shape\n",
    "        ax.set_ylabel(\n",
    "            f'{layer_name}\\n{shp[0]}x{shp[1]}',\n",
    "            rotation=0, labelpad=40\n",
    "        )\n",
    "        xlabel1 = '' if y is None else f'\\nLabel: {y[j]}'\n",
    "        xlabel2 = '' if yhat is None else f'\\nPredicted: {yhat[j]}'\n",
    "        xlabel = f'{xlabel1}{xlabel2}'\n",
    "        if len(xlabel):\n",
    "            ax.set_xlabel(xlabel, fontsize=12)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "        # Plots weight as an image\n",
    "        ax.imshow(\n",
    "            np.atleast_2d(image.squeeze()),\n",
    "            cmap='gray', \n",
    "            vmin=minv, \n",
    "            vmax=maxv\n",
    "        )\n",
    "    return\n",
    "\n",
    "setattr(StepByStep, '_visualize_tensors', _visualize_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cat(object):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    @staticmethod\n",
    "    def meow():\n",
    "        print('Meow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cat.meow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_filter = model_cnn1.conv1.weight.data.cpu().numpy()\n",
    "weights_filter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_filters(self, layer_name, **kwargs):\n",
    "    try:\n",
    "        # Gets the layer object from the model\n",
    "        layer = self.model\n",
    "        for name in layer_name.split('.'):\n",
    "            layer = getattr(layer, name)\n",
    "        # We are only looking at filters for 2D convolutions\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            # Takes the weight information\n",
    "            weights = layer.weight.data.cpu().numpy()\n",
    "            # weights -> (channels_out (filter), channels_in, H, W)\n",
    "            n_filters, n_channels, _, _ = weights.shape\n",
    "\n",
    "            # Builds a figure\n",
    "            size = (2 * n_channels + 2, 2 * n_filters)\n",
    "            fig, axes = plt.subplots(n_filters, n_channels, \n",
    "                                     figsize=size)\n",
    "            axes = np.atleast_2d(axes)\n",
    "            axes = axes.reshape(n_filters, n_channels)\n",
    "            # For each channel_out (filter)\n",
    "            for i in range(n_filters):    \n",
    "                StepByStep._visualize_tensors(\n",
    "                    axes[i, :],\n",
    "                    weights[i],\n",
    "                    layer_name=f'Filter #{i}', \n",
    "                    title='Channel'\n",
    "                )\n",
    "                    \n",
    "            for ax in axes.flat:\n",
    "                ax.label_outer()\n",
    "\n",
    "            fig.tight_layout()\n",
    "            return fig\n",
    "    except AttributeError:\n",
    "        return\n",
    "    \n",
    "setattr(StepByStep, 'visualize_filters', visualize_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sbs_cnn1.visualize_filters('conv1', cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model = nn.Linear(1, 1)\n",
    "\n",
    "dummy_list = []\n",
    "\n",
    "def dummy_hook(layer, inputs, outputs):\n",
    "    dummy_list.append((layer, inputs, outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_handle = dummy_model.register_forward_hook(dummy_hook)\n",
    "dummy_handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_x = torch.tensor([0.3])\n",
    "dummy_model.forward(dummy_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model(dummy_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = list(sbs_cnn1.model.named_modules())\n",
    "modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = {layer: name for name, layer in modules[1:]}\n",
    "layer_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization = {}\n",
    "\n",
    "def hook_fn(layer, inputs, outputs):\n",
    "    name = layer_names[layer]\n",
    "    visualization[name] = outputs.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_to_hook = ['conv1', 'relu1', 'maxp1', 'flatten', 'fc1', 'relu2', 'fc2']\n",
    "\n",
    "handles = {}\n",
    "\n",
    "for name, layer in modules:\n",
    "    if name in layers_to_hook:\n",
    "        handles[name] = layer.register_forward_hook(hook_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_batch, labels_batch = iter(val_loader).next()\n",
    "logits = sbs_cnn1.predict(images_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for handle in handles.values():\n",
    "    handle.remove()\n",
    "handles = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setattr(StepByStep, 'visualization', {})\n",
    "setattr(StepByStep, 'handles', {})\n",
    "\n",
    "def attach_hooks(self, layers_to_hook, hook_fn=None):\n",
    "    # Clear any previous values\n",
    "    self.visualization = {}\n",
    "    # Creates the dictionary to map layer objects to their names\n",
    "    modules = list(self.model.named_modules())\n",
    "    layer_names = {layer: name for name, layer in modules[1:]}\n",
    "\n",
    "    if hook_fn is None:\n",
    "        # Hook function to be attached to the forward pass\n",
    "        def hook_fn(layer, inputs, outputs):\n",
    "            # Gets the layer name\n",
    "            name = layer_names[layer]\n",
    "            # Detaches outputs\n",
    "            values = outputs.detach().cpu().numpy()\n",
    "            # Since the hook function may be called multiple times\n",
    "            # for example, if we make predictions for multiple mini-batches\n",
    "            # it concatenates the results\n",
    "            if self.visualization[name] is None:\n",
    "                self.visualization[name] = values\n",
    "            else:\n",
    "                self.visualization[name] = np.concatenate([self.visualization[name], values])\n",
    "\n",
    "    for name, layer in modules:\n",
    "        # If the layer is in our list\n",
    "        if name in layers_to_hook:\n",
    "            # Initializes the corresponding key in the dictionary\n",
    "            self.visualization[name] = None\n",
    "            # Register the forward hook and keep the handle in another dict\n",
    "            self.handles[name] = layer.register_forward_hook(hook_fn)\n",
    "\n",
    "def remove_hooks(self):\n",
    "    # Loops through all hooks and removes them\n",
    "    for handle in self.handles.values():\n",
    "        handle.remove()\n",
    "    # Clear the dict, as all hooks have been removed\n",
    "    self.handles = {}\n",
    "    \n",
    "setattr(StepByStep, 'attach_hooks', attach_hooks)\n",
    "setattr(StepByStep, 'remove_hooks', remove_hooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbs_cnn1.attach_hooks(layers_to_hook=['conv1', 'relu1', 'maxp1', 'flatten', 'fc1', 'relu2', 'fc2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_batch, labels_batch = iter(val_loader).next()\n",
    "logits = sbs_cnn1.predict(images_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbs_cnn1.remove_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = np.argmax(logits, 1)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Feature Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_images(images_batch.squeeze(), labels_batch.squeeze(), n_plot=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_outputs(self, layers, n_images=10, y=None, yhat=None):\n",
    "    layers = filter(lambda l: l in self.visualization.keys(), layers)\n",
    "    layers = list(layers)\n",
    "    shapes = [self.visualization[layer].shape for layer in layers]\n",
    "    n_rows = [shape[1] if len(shape) == 4 else 1 \n",
    "              for shape in shapes]\n",
    "    total_rows = np.sum(n_rows)\n",
    "\n",
    "    fig, axes = plt.subplots(total_rows, n_images, \n",
    "                             figsize=(1.5*n_images, 1.5*total_rows))\n",
    "    axes = np.atleast_2d(axes).reshape(total_rows, n_images)\n",
    "    \n",
    "    # Loops through the layers, one layer per row of subplots\n",
    "    row = 0\n",
    "    for i, layer in enumerate(layers):\n",
    "        start_row = row\n",
    "        # Takes the produced feature maps for that layer\n",
    "        output = self.visualization[layer]\n",
    "            \n",
    "        is_vector = len(output.shape) == 2\n",
    "        \n",
    "        for j in range(n_rows[i]):\n",
    "            StepByStep._visualize_tensors(\n",
    "                axes[row, :],\n",
    "                output if is_vector else output[:, j].squeeze(),\n",
    "                y, \n",
    "                yhat, \n",
    "                layer_name=layers[i] \\\n",
    "                           if is_vector \\\n",
    "                           else f'{layers[i]}\\nfil#{row-start_row}',\n",
    "                title='Image' if (row == 0) else None\n",
    "            )\n",
    "            row += 1\n",
    "            \n",
    "    for ax in axes.flat:\n",
    "        ax.label_outer()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "setattr(StepByStep, 'visualize_outputs', visualize_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer_layers = ['conv1', 'relu1', 'maxp1', 'flatten']\n",
    "\n",
    "with plt.style.context('seaborn-white'):\n",
    "    fig = sbs_cnn1.visualize_outputs(featurizer_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Classifier Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_layers = ['fc1', 'relu2', 'fc2']\n",
    "\n",
    "with plt.style.context('seaborn-white'):\n",
    "    fig = sbs_cnn1.visualize_outputs(classifier_layers, y=labels_batch, yhat=predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(self, x, y, threshold=.5):\n",
    "    self.model.eval()\n",
    "    yhat = self.model(x.to(self.device))\n",
    "    y = y.to(self.device)\n",
    "    self.model.train()\n",
    "    \n",
    "    # We get the size of the batch and the number of classes \n",
    "    # (only 1, if it is binary)\n",
    "    n_samples, n_dims = yhat.shape\n",
    "    if n_dims > 1:        \n",
    "        # In a multiclass classification, the biggest logit\n",
    "        # always wins, so we don't bother getting probabilities\n",
    "        \n",
    "        # This is PyTorch's version of argmax, \n",
    "        # but it returns a tuple: (max value, index of max value)\n",
    "        _, predicted = torch.max(yhat, 1)\n",
    "    else:\n",
    "        n_dims += 1\n",
    "        # In binary classification, we NEED to check if the\n",
    "        # last layer is a sigmoid (and then it produces probs)\n",
    "        if isinstance(self.model, nn.Sequential) and \\\n",
    "           isinstance(self.model[-1], nn.Sigmoid):\n",
    "            predicted = (yhat > threshold).long()\n",
    "        # or something else (logits), which we need to convert\n",
    "        # using a sigmoid\n",
    "        else:\n",
    "            predicted = (F.sigmoid(yhat) > threshold).long()\n",
    "    \n",
    "    # How many samples got classified correctly for each class\n",
    "    result = []\n",
    "    for c in range(n_dims):\n",
    "        n_class = (y == c).sum().item()\n",
    "        n_correct = (predicted[y == c] == c).sum().item()\n",
    "        result.append((n_correct, n_class))\n",
    "    return torch.tensor(result)\n",
    "\n",
    "setattr(StepByStep, 'correct', correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbs_cnn1.correct(images_batch, labels_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loader Apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def loader_apply(loader, func, reduce='sum'):\n",
    "    results = [func(x, y) for i, (x, y) in enumerate(loader)]\n",
    "    results = torch.stack(results, axis=0)\n",
    "\n",
    "    if reduce == 'sum':\n",
    "        results = results.sum(axis=0)\n",
    "    elif reduce == 'mean':\n",
    "        results = results.float().mean(axis=0)\n",
    "    \n",
    "    return results\n",
    "\n",
    "setattr(StepByStep, 'loader_apply', loader_apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StepByStep.loader_apply(sbs_cnn1.val_loader, sbs_cnn1.correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds tensors from numpy arrays BEFORE split\n",
    "# Modifies the scale of pixel values from [0, 255] to [0, 1]\n",
    "x_tensor = torch.as_tensor(images / 255).float()\n",
    "y_tensor = torch.as_tensor(labels).long()\n",
    "\n",
    "# Uses index_splitter to generate indices for training and\n",
    "# validation sets\n",
    "train_idx, val_idx = index_splitter(len(x_tensor), [80, 20])\n",
    "# Uses indices to perform the split\n",
    "x_train_tensor = x_tensor[train_idx]\n",
    "y_train_tensor = y_tensor[train_idx]\n",
    "x_val_tensor = x_tensor[val_idx]\n",
    "y_val_tensor = y_tensor[val_idx]\n",
    "\n",
    "# We're not doing any data augmentation now\n",
    "train_composer = Compose([Normalize(mean=(.5,), std=(.5,))])\n",
    "val_composer = Compose([Normalize(mean=(.5,), std=(.5,))])\n",
    "\n",
    "# Uses custom dataset to apply composed transforms to each set\n",
    "train_dataset = TransformedTensorDataset(x_train_tensor, y_train_tensor, transform=train_composer)\n",
    "val_dataset = TransformedTensorDataset(x_val_tensor, y_val_tensor, transform=val_composer)\n",
    "\n",
    "# Builds a weighted random sampler to handle imbalanced classes\n",
    "sampler = make_balanced_sampler(y_train_tensor)\n",
    "\n",
    "# Uses sampler in the training set to get a balanced data loader\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16, sampler=sampler)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "model_cnn1 = nn.Sequential()\n",
    "\n",
    "# Featurizer\n",
    "# Block 1: 1@10x10 -> n_channels@8x8 -> n_channels@4x4\n",
    "n_channels = 1\n",
    "model_cnn1.add_module('conv1', nn.Conv2d(in_channels=1, out_channels=n_channels, kernel_size=3))\n",
    "model_cnn1.add_module('relu1', nn.ReLU())\n",
    "model_cnn1.add_module('maxp1', nn.MaxPool2d(kernel_size=2))\n",
    "# Flattening: n_channels * 4 * 4\n",
    "model_cnn1.add_module('flatten', nn.Flatten())\n",
    "\n",
    "# Classification\n",
    "# Hidden Layer\n",
    "model_cnn1.add_module('fc1', nn.Linear(in_features=n_channels*4*4, out_features=10))\n",
    "model_cnn1.add_module('relu2', nn.ReLU())\n",
    "# Output Layer\n",
    "model_cnn1.add_module('fc2', nn.Linear(in_features=10, out_features=3))\n",
    "\n",
    "lr = 0.1\n",
    "multi_loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer_cnn1 = optim.SGD(model_cnn1.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbs_cnn1 = StepByStep(model_cnn1, multi_loss_fn, optimizer_cnn1)\n",
    "sbs_cnn1.set_loaders(train_loader, val_loader)\n",
    "sbs_cnn1.train(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_filters = sbs_cnn1.visualize_filters('conv1', cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturing Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer_layers = ['conv1', 'relu1', 'maxp1', 'flatten']\n",
    "classifier_layers = ['fc1', 'relu2', 'fc2']\n",
    "\n",
    "sbs_cnn1.attach_hooks(layers_to_hook=featurizer_layers + classifier_layers)\n",
    "\n",
    "images_batch, labels_batch = iter(val_loader).next()\n",
    "logits = sbs_cnn1.predict(images_batch)\n",
    "predicted = np.argmax(logits, 1)\n",
    "\n",
    "sbs_cnn1.remove_hooks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Feature Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-white'):\n",
    "    fig_maps1 = sbs_cnn1.visualize_outputs(featurizer_layers)\n",
    "    fig_maps2 = sbs_cnn1.visualize_outputs(classifier_layers, y=labels_batch, yhat=predicted)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StepByStep.loader_apply(sbs_cnn1.val_loader, sbs_cnn1.correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
